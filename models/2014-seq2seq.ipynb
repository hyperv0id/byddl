{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c34b896",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.12/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b501729e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device=cuda\n"
     ]
    }
   ],
   "source": [
    "MAX_TOKEN=150\n",
    "batch_size = 64\n",
    "num_epochs = 10\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"device={device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c48bc60c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "163 44\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eng</th>\n",
       "      <th>zht</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20297</th>\n",
       "      <td>The airplane made a safe landing.</td>\n",
       "      <td>這架飛機安全著陸了。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25504</th>\n",
       "      <td>Tom is the coolest person in the world.</td>\n",
       "      <td>汤姆是世界上最酷的人。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17131</th>\n",
       "      <td>I hate terrorist organizations.</td>\n",
       "      <td>我痛恨恐怖主义组织。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>748</th>\n",
       "      <td>I handled it.</td>\n",
       "      <td>我處理了。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21603</th>\n",
       "      <td>This homework is difficult for me.</td>\n",
       "      <td>這個家庭作業對我來說很困難。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27365</th>\n",
       "      <td>We still have many other things to discuss.</td>\n",
       "      <td>我们还有许多别的事情要讨论。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4133</th>\n",
       "      <td>No one will see us.</td>\n",
       "      <td>沒有人會看到我們。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18927</th>\n",
       "      <td>She made the same mistake again.</td>\n",
       "      <td>她又犯了同樣的錯誤了。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9001</th>\n",
       "      <td>The sheet is on the bed.</td>\n",
       "      <td>床单在床上。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6373</th>\n",
       "      <td>Does he speak English?</td>\n",
       "      <td>他會講英語嗎？</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>30919 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               eng             zht\n",
       "20297            The airplane made a safe landing.      這架飛機安全著陸了。\n",
       "25504      Tom is the coolest person in the world.     汤姆是世界上最酷的人。\n",
       "17131              I hate terrorist organizations.      我痛恨恐怖主义组织。\n",
       "748                                  I handled it.           我處理了。\n",
       "21603           This homework is difficult for me.  這個家庭作業對我來說很困難。\n",
       "...                                            ...             ...\n",
       "27365  We still have many other things to discuss.  我们还有许多别的事情要讨论。\n",
       "4133                           No one will see us.       沒有人會看到我們。\n",
       "18927             She made the same mistake again.     她又犯了同樣的錯誤了。\n",
       "9001                      The sheet is on the bed.          床单在床上。\n",
       "6373                        Does he speak English?         他會講英語嗎？\n",
       "\n",
       "[30919 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# link: http://www.manythings.org/anki/\n",
    "# 这里使用的是 中英翻译\n",
    "df = pd.read_csv(\"./tatoeba_dataset/cmn.txt\", sep='\\t', header=None)\n",
    "df[['eng', 'zht']] = df[[0, 1]]\n",
    "df = df[['eng', 'zht']]\n",
    "df = df.sample(frac=1)\n",
    "max_eng_len = df['eng'].map(lambda x:len(x)).max()\n",
    "max_zht_len = df['zht'].map(lambda x:len(x)).max()\n",
    "print(max_eng_len, max_zht_len)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "75cc21fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer = AutoTokenizer.from_pretrained(\"bert-base-multilingual-cased\")\n",
    "# msg = \"床前明月光，疑是地上霜\"\n",
    "# tokenized = tokenizer(msg)\n",
    "# tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "66168dfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HF_ENDPOINT set to: https://hf-mirror.com\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "hf_mirror_url = \"https://hf-mirror.com\"\n",
    "os.environ['HF_ENDPOINT'] = hf_mirror_url\n",
    "print(f\"HF_ENDPOINT set to: {os.environ['HF_ENDPOINT']}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ce4f689d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.12/site-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "单词表大小，en=30522, zh=21128\n"
     ]
    }
   ],
   "source": [
    "from typing import Tuple\n",
    "from pandas import DataFrame\n",
    "from torch import Tensor\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "\n",
    "\n",
    "class CMNDataset(Dataset):\n",
    "    def __init__(self, df: DataFrame, tokenizer_en, tokenizer_zh, source='eng'):\n",
    "        super(CMNDataset, self).__init__()\n",
    "        self.src_texts = df[\"eng\"].to_list()\n",
    "        self.tgt_texts = df[\"zht\"].to_list()\n",
    "        if source != 'eng':\n",
    "            self.src_texts, self.tgt_texts = self.tgt_texts, self.src_texts\n",
    "            \n",
    "        self.src_tokenizer = tokenizer_en\n",
    "        self.tgt_tokenizer = tokenizer_zh\n",
    "        self.tokenize_f = lambda tokenizer, text: tokenizer(\n",
    "            text,\n",
    "            max_length=MAX_TOKEN,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )[\"input_ids\"].squeeze(0)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.src_texts)\n",
    "\n",
    "    def __getitem__(self, index) -> Tuple[Tensor, Tensor]:\n",
    "        return self.tokenize_f(self.src_tokenizer, self.src_texts[index]), self.tokenize_f(self.tgt_tokenizer, self.tgt_texts[index])\n",
    "\n",
    "df = df.sample(frac=1)\n",
    "total = len(df)\n",
    "\n",
    "tokenizer_en = AutoTokenizer.from_pretrained(\"bert-base-uncased\", )\n",
    "tokenizer_zh = AutoTokenizer.from_pretrained(\"bert-base-chinese\")\n",
    "print(f\"单词表大小，en={tokenizer_en.vocab_size}, zh={tokenizer_zh.vocab_size}\")\n",
    "\n",
    "train_ds = CMNDataset(df[: int(total * 0.8)], tokenizer_en, tokenizer_zh)\n",
    "test_ds = CMNDataset(df[int(total * 0.8) :], tokenizer_en, tokenizer_zh)\n",
    "\n",
    "train_iter = DataLoader(train_ds, batch_size, shuffle=True)\n",
    "test_iter = DataLoader(test_ds, batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8623297d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30919"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac7cd6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        tokenizer_enc,\n",
    "        tokenizer_dec,\n",
    "        embed_size=512,\n",
    "        hidden_size=1024,\n",
    "        num_layers=2,\n",
    "        dropout=0.1,\n",
    "    ):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.tokenizer_enc = tokenizer_enc\n",
    "        self.tokenizer_dec = tokenizer_dec\n",
    "        self.embed_size = embed_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout_p = dropout\n",
    "\n",
    "        self.embedding_enc = nn.Embedding(\n",
    "            num_embeddings=self.tokenizer_enc.vocab_size,\n",
    "            embedding_dim=self.embed_size,\n",
    "            padding_idx=tokenizer_enc.pad_token_id,\n",
    "        )\n",
    "        self.encoder = nn.GRU(\n",
    "            self.embed_size,\n",
    "            self.hidden_size,\n",
    "            self.num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=self.dropout_p if self.num_layers > 1 else 0,\n",
    "        )\n",
    "        self.embedding_dec = nn.Embedding(\n",
    "            num_embeddings=self.tokenizer_dec.vocab_size,\n",
    "            embedding_dim=self.embed_size,\n",
    "            padding_idx=tokenizer_dec.pad_token_id,\n",
    "        )\n",
    "        self.decoder = nn.GRU(\n",
    "            self.embed_size,\n",
    "            self.hidden_size,\n",
    "            self.num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=self.dropout_p if self.num_layers > 1 else 0,\n",
    "        )\n",
    "        self.fc_out = nn.Linear(self.hidden_size, self.tokenizer_dec.vocab_size)\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        # src shape=(bs, seq_len)\n",
    "\n",
    "        embedded_src = self.embedding_enc(src)  # (bs, embed_size)\n",
    "        _, hidden = self.encoder(embedded_src)  # (bs, hidden)\n",
    "        embedded_tgt = self.embedding_dec(tgt)\n",
    "        output_dec, _ = self.decoder(embedded_tgt, hidden)  #\n",
    "        predictions = self.fc_out(output_dec)  # (bs, vocab_size)\n",
    "        return predictions\n",
    "\n",
    "    def generate(self, src: str, max_len=150):\n",
    "        self.eval()\n",
    "\n",
    "        src_tokens = self.tokenizer_enc(\n",
    "            src,\n",
    "            max_length=max_len,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )[\"input_ids\"].to(device)\n",
    "\n",
    "        # b. 编码源句子，获取上下文向量\n",
    "        with torch.no_grad():\n",
    "            embedded_src = self.embedding_enc(src_tokens)\n",
    "            _, hidden = self.encoder(embedded_src)\n",
    "\n",
    "        tgt_tokens = [self.tokenizer_dec.cls_token_id]\n",
    "\n",
    "        for _ in range(max_len):\n",
    "            tgt_input_tensor = torch.tensor([tgt_tokens[-1]], dtype=torch.long).unsqueeze(0).to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                embedded_tgt = self.embedding_dec(tgt_input_tensor)\n",
    "                output_dec, hidden = self.decoder(embedded_tgt, hidden)\n",
    "                prediction = self.fc_out(output_dec)\n",
    "\n",
    "            # 贪心，从词汇表概率中选择概率最大的词\n",
    "            predicted_token_id = prediction.argmax(2).item()\n",
    "\n",
    "            if predicted_token_id == self.tokenizer_dec.sep_token_id:\n",
    "                break\n",
    "\n",
    "            tgt_tokens.append(predicted_token_id)\n",
    "\n",
    "        return self.tokenizer_dec.decode(tgt_tokens[1:], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7aa06fb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "模型总参数: 55686792\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 387/387 [01:38<00:00,  3.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 finished, Average Loss: 0.8981\n",
      "Epoch 1 Test Loss: 0.3698\n",
      "我 我 不 的 。\t我 不 是 的 。\t我 我 不 的 。\t我 我 不 的 。\t我 我 不 的 。\t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 387/387 [01:38<00:00,  3.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 finished, Average Loss: 0.3587\n",
      "Epoch 2 Test Loss: 0.3433\n",
      "我 們 在 我 的 。\t你 的 是 我 的 的 。\t我 們 在 我 的 。\t我 們 在 我 的 。\t我 們 在 我 的 。\t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 387/387 [01:39<00:00,  3.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 finished, Average Loss: 0.3273\n",
      "Epoch 3 Test Loss: 0.3091\n",
      "我 不 是 我 的 。\t你 能 在 哪 ？\t他 的 人 是 一 个 人 。\t我 們 在 我 的 。\t我 不 是 我 的 。\t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 387/387 [01:39<00:00,  3.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 finished, Average Loss: 0.2963\n",
      "Epoch 4 Test Loss: 0.2842\n",
      "我 想 要 你 的 。\t你 在 哪 裡 ？\t这 是 我 的 。\t我 们 在 這 裡 。\t我 不 知 道 他 的 。\t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 387/387 [01:39<00:00,  3.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 finished, Average Loss: 0.2727\n",
      "Epoch 5 Test Loss: 0.2652\n",
      "我 想 要 你 。\t你 们 什 么 ？\t这 是 一 个 人 。\t我 们 要 去 了 。\t我 的 房 子 。\t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 387/387 [01:39<00:00,  3.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 finished, Average Loss: 0.2539\n",
      "Epoch 6 Test Loss: 0.2506\n",
      "我 想 要 你 。\t你 们 什 么 ？\t这 是 一 个 人 。\t我 们 去 了 一 個 小 时 。\t我 的 房 子 是 我 的 。\t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 387/387 [01:40<00:00,  3.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 finished, Average Loss: 0.2381\n",
      "Epoch 7 Test Loss: 0.2391\n",
      "我 想 你 。\t你 们 什 么 ？\t这 是 一 个 人 。\t我 們 去 了 一 個 小 时 。\t我 的 房 子 是 汤 姆 。\t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 387/387 [01:40<00:00,  3.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 finished, Average Loss: 0.2242\n",
      "Epoch 8 Test Loss: 0.2288\n",
      "我 想 你 。\t你 们 什 么 ？\t这 是 一 个 人 。\t我 们 去 学 校 。\t我 的 房 子 是 汤 姆 。\t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 387/387 [01:39<00:00,  3.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 finished, Average Loss: 0.2116\n",
      "Epoch 9 Test Loss: 0.2202\n",
      "我 喜 欢 你 。\t你 们 怎 么 样 ？\t这 是 一 个 好 的 。\t我 們 去 看 電 影 。\t我 的 房 子 是 汤 姆 。\t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 387/387 [01:39<00:00,  3.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 finished, Average Loss: 0.2000\n",
      "Epoch 10 Test Loss: 0.2130\n",
      "我 喜 欢 你 。\t你 們 的 事 情 吗 ？\t这 是 一 个 好 的 。\t我 們 去 学 校 。\t我 的 房 子 是 汤 姆 。\t\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "model = Seq2Seq(\n",
    "    tokenizer_enc=tokenizer_en, tokenizer_dec=tokenizer_zh, hidden_size=768, dropout=0.2\n",
    ").to(device)\n",
    "print(\"模型总参数:\", sum(p.numel() for p in model.parameters()))\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "loss_func = torch.nn.CrossEntropyLoss()\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for x, y in tqdm(train_iter):\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        pred = model(x, y[:, :-1])\n",
    "\n",
    "        loss_val = loss_func(pred.permute(0, 2, 1), y[:, 1:])\n",
    "        loss_val.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        total_loss += loss_val.item()\n",
    "    print(f\"Epoch {epoch+1} finished, Average Loss: {total_loss / len(train_iter):.4f}\")\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        test_loss = 0\n",
    "        for x_test, y_test in test_iter:\n",
    "            x_test = x_test.to(device)\n",
    "            y_test = y_test.to(device)\n",
    "\n",
    "            pred_test = model(x_test, y_test[:, :-1])\n",
    "            loss_val_test = loss_func(pred_test.permute(0, 2, 1), y_test[:, 1:])\n",
    "            test_loss += loss_val_test.item()\n",
    "\n",
    "        print(f\"Epoch {epoch+1} Test Loss: {test_loss / len(test_iter):.4f}\")\n",
    "    test_sentences = [\n",
    "        \"I love you.\",\n",
    "        \"How are you?\",\n",
    "        \"This is a book.\",\n",
    "        \"Let's go to school.\",\n",
    "        \"My name is Tom.\"\n",
    "    ]\n",
    "    for sentence in test_sentences:\n",
    "        translation = model.generate(sentence)\n",
    "        print(f\"{translation}\", end='\\t')\n",
    "    print()\n",
    "    torch.save(model.state_dict(), f'seq2seq_weights_ep{epoch+1}.pth')\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
